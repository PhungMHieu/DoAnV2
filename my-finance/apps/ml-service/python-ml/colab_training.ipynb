{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Finance - Category Classification Model\n",
    "## TF-IDF + SVM for Vietnamese Transaction Text\n",
    "\n",
    "**Cách upload file `training_data.json`:**\n",
    "1. Upload lên Google Drive\n",
    "2. Chạy cell mount Drive bên dưới\n",
    "3. Hoặc dùng URL nếu file đã public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTION 1: Mount Google Drive (Recommended) ===\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy file từ Drive vào Colab\n",
    "# Thay đổi path phù hợp với vị trí file của bạn trên Drive\n",
    "!cp \"/content/drive/My Drive/training_data.json\" .\n",
    "\n",
    "# === OPTION 2: Direct upload (có thể lỗi với file lớn) ===\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing for Vietnamese\n",
    "TEENCODE_MAP = {\n",
    "    \"k\": \"không\", \"ko\": \"không\", \"k0\": \"không\",\n",
    "    \"dc\": \"được\", \"đc\": \"được\",\n",
    "    \"vs\": \"với\", \"j\": \"gì\", \"z\": \"vậy\", \"r\": \"rồi\",\n",
    "    \"cf\": \"cafe\", \"coffe\": \"coffee\", \"cofee\": \"coffee\",\n",
    "}\n",
    "\n",
    "TYPO_MAP = {\n",
    "    \"grap\": \"grab\", \"grabs\": \"grab\",\n",
    "    \"shoppee\": \"shopee\", \"lazda\": \"lazada\",\n",
    "    \"hoá đơn\": \"hóa đơn\", \"cà fê\": \"cà phê\",\n",
    "    \"ca phe\": \"cà phê\", \"tra sua\": \"trà sữa\",\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Fix typos\n",
    "    for typo, fix in TYPO_MAP.items():\n",
    "        text = re.sub(re.escape(typo), fix, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Fix teencode\n",
    "    words = text.split()\n",
    "    words = [TEENCODE_MAP.get(w, w) for w in words]\n",
    "    text = \" \".join(words)\n",
    "    \n",
    "    # Remove special chars but keep Vietnamese\n",
    "    text = re.sub(r\"[^\\w\\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(preprocess_text(\"Ăn phở 50k\"))\n",
    "print(preprocess_text(\"đi grap 30k\"))\n",
    "print(preprocess_text(\"cf vs bạn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "with open('training_data.json', 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Total records: {len(raw_data)}\")\n",
    "print(f\"Sample: {raw_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data\n",
    "CATEGORIES = [\n",
    "    \"income\", \"food\", \"transportation\", \"entertainment\", \"shopping\",\n",
    "    \"health\", \"education\", \"utilities\", \"home\", \"personal\",\n",
    "    \"travel\", \"investment\", \"family\", \"houseware\", \"donation\", \"charity\", \"other\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for record in raw_data:\n",
    "    text = record.get('text', '')\n",
    "    category = record.get('correctedCategory', record.get('category', ''))\n",
    "    \n",
    "    if text and category and category in CATEGORIES:\n",
    "        texts.append(preprocess_text(text))\n",
    "        labels.append(category)\n",
    "\n",
    "print(f\"Valid samples: {len(texts)}\")\n",
    "\n",
    "# Category distribution\n",
    "category_counts = Counter(labels)\n",
    "print(\"\\nCategory distribution:\")\n",
    "for cat, count in category_counts.most_common():\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET SUMMARY TABLE ===\n",
    "\n",
    "df = pd.DataFrame({'text': texts, 'category': labels})\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Summary table\n",
    "summary = df.groupby('category').agg(\n",
    "    samples=('text', 'count'),\n",
    "    avg_words=('word_count', 'mean'),\n",
    "    min_words=('word_count', 'min'),\n",
    "    max_words=('word_count', 'max')\n",
    ").round(1)\n",
    "\n",
    "summary['percent'] = (summary['samples'] / len(df) * 100).round(1)\n",
    "summary = summary[['samples', 'percent', 'avg_words', 'min_words', 'max_words']]\n",
    "summary.columns = ['Samples', '%', 'Avg Words', 'Min', 'Max']\n",
    "summary = summary.sort_values('Samples', ascending=False)\n",
    "\n",
    "print(f\"Total: {len(df)} samples, {len(category_counts)} categories\\n\")\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis - Kiểm tra imbalanced data\n",
    "print(\"=== DATA QUALITY CHECK ===\\n\")\n",
    "\n",
    "# 1. Check imbalance ratio\n",
    "max_count = max(category_counts.values())\n",
    "min_count = min(category_counts.values())\n",
    "print(f\"Imbalance ratio: {max_count/min_count:.1f}x\")\n",
    "print(f\"Max category: {max(category_counts, key=category_counts.get)} ({max_count})\")\n",
    "print(f\"Min category: {min(category_counts, key=category_counts.get)} ({min_count})\")\n",
    "\n",
    "# 2. Check text length distribution\n",
    "text_lengths = [len(t.split()) for t in texts]\n",
    "print(f\"\\nText length (words): mean={np.mean(text_lengths):.1f}, min={min(text_lengths)}, max={max(text_lengths)}\")\n",
    "\n",
    "# 3. Categories cần thêm data\n",
    "print(\"\\nCategories cần thêm data (< 500 samples):\")\n",
    "for cat, count in category_counts.items():\n",
    "    if count < 500:\n",
    "        print(f\"  {cat}: {count} (cần thêm ~{500-count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "TFIDF_PARAMS = {\n",
    "    \"max_features\": 5000,\n",
    "    \"ngram_range\": (1, 3),\n",
    "    \"min_df\": 2,\n",
    "    \"max_df\": 0.95,\n",
    "    \"sublinear_tf\": True,\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(**TFIDF_PARAMS)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sánh nhiều models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    'SVM Linear': SVC(kernel='linear', probability=True, class_weight='balanced'),\n",
    "    'SVM RBF': SVC(kernel='rbf', probability=True, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1),\n",
    "}\n",
    "\n",
    "print(\"Comparing models with 5-fold CV:\\n\")\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean F1': scores.mean(),\n",
    "        'Std': scores.std()\n",
    "    })\n",
    "    print(f\"{name:20} F1: {scores.mean():.2%} (+/- {scores.std():.2%})\")\n",
    "\n",
    "# Visualize\n",
    "results_df = pd.DataFrame(results).sort_values('Mean F1', ascending=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(results_df['Model'], results_df['Mean F1'], xerr=results_df['Std'], color='steelblue')\n",
    "plt.xlabel('F1 Score (weighted)')\n",
    "plt.title('Model Comparison')\n",
    "plt.xlim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning với GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Tham số cần tìm kiếm\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],  # Chỉ dùng cho rbf\n",
    "}\n",
    "\n",
    "# GridSearch với cross-validation 5-fold\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(probability=True, class_weight='balanced'),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Searching best parameters...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.2%}\")\n",
    "\n",
    "# Dùng model tốt nhất\n",
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Cross-validation: {cv_scores.mean():.2%} (+/- {cv_scores.std():.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "def predict(text):\n",
    "    processed = preprocess_text(text)\n",
    "    X_new = vectorizer.transform([processed])\n",
    "    proba = model.predict_proba(X_new)[0]\n",
    "    top_idx = np.argsort(proba)[::-1][:3]\n",
    "    \n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Preprocessed: {processed}\")\n",
    "    for idx in top_idx:\n",
    "        cat = label_encoder.inverse_transform([idx])[0]\n",
    "        conf = proba[idx]\n",
    "        print(f\"  {cat}: {conf:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Test\n",
    "predict(\"ăn phở sáng 50k\")\n",
    "predict(\"đi grab về nhà\")\n",
    "predict(\"mua quần áo shopee\")\n",
    "predict(\"tiền điện tháng 12\")\n",
    "predict(\"lương tháng 1\")\n",
    "predict(\"cà phê với bạn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORT FOR THESIS/REPORT ===\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THÔNG TIN CHO ĐỒ ÁN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Dataset Overview\n",
    "print(\"\\n1. TỔNG QUAN DATASET\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Tổng số mẫu:     {len(df):,}\")\n",
    "print(f\"   Số categories:   {len(category_counts)}\")\n",
    "print(f\"   Ngôn ngữ:        Tiếng Việt\")\n",
    "\n",
    "# 2. Category Distribution Table\n",
    "print(\"\\n2. PHÂN BỐ DỮ LIỆU THEO CATEGORY\")\n",
    "print(\"-\" * 40)\n",
    "print(summary.to_string())\n",
    "\n",
    "# 3. Preprocessing\n",
    "print(\"\\n3. TIỀN XỬ LÝ VĂN BẢN\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   - Unicode normalization (NFC)\")\n",
    "print(\"   - Chuyển lowercase\")\n",
    "print(\"   - Chuẩn hóa teencode (k→không, dc→được, cf→cafe, ...)\")\n",
    "print(\"   - Sửa lỗi chính tả (grap→grab, shoppee→shopee, ...)\")\n",
    "print(\"   - Loại bỏ ký tự đặc biệt, giữ tiếng Việt\")\n",
    "\n",
    "# 4. Feature Extraction\n",
    "print(\"\\n4. TRÍCH XUẤT ĐẶC TRƯNG (TF-IDF)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   N-gram range:    (1, 3)\")\n",
    "print(f\"   Max features:    5,000\")\n",
    "print(f\"   Min document freq: 2\")\n",
    "print(f\"   Max document freq: 95%\")\n",
    "print(f\"   Sublinear TF:    True\")\n",
    "print(f\"   Số features thực tế: {X.shape[1]:,}\")\n",
    "\n",
    "# 5. Model Info\n",
    "print(\"\\n5. MÔ HÌNH PHÂN LOẠI\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Algorithm:       {type(model).__name__}\")\n",
    "if hasattr(model, 'kernel'):\n",
    "    print(f\"   Kernel:          {model.kernel}\")\n",
    "if hasattr(model, 'C'):\n",
    "    print(f\"   C:               {model.C}\")\n",
    "print(f\"   Class weight:    balanced\")\n",
    "\n",
    "# 6. Results\n",
    "print(\"\\n6. KẾT QUẢ\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Train/Test split: 80/20\")\n",
    "print(f\"   Accuracy:        {accuracy:.2%}\")\n",
    "print(f\"   Cross-validation: {cv_scores.mean():.2%} (+/- {cv_scores.std():.2%})\")\n",
    "\n",
    "# 7. Classification Report\n",
    "print(\"\\n7. CLASSIFICATION REPORT\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features per category\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Top 10 features per category:\")\n",
    "for i, category in enumerate(label_encoder.classes_):\n",
    "    # Get indices of samples in this category\n",
    "    cat_mask = (y == i)\n",
    "    if cat_mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Mean TF-IDF for this category\n",
    "    cat_tfidf = X[cat_mask].mean(axis=0).A1\n",
    "    top_indices = cat_tfidf.argsort()[::-1][:10]\n",
    "    top_features = [(feature_names[idx], cat_tfidf[idx]) for idx in top_indices]\n",
    "    \n",
    "    print(f\"\\n{category}:\")\n",
    "    for feat, score in top_features:\n",
    "        print(f\"  {feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model (optional - download to local)\n",
    "import joblib\n",
    "\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(model, 'svm_classifier.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "\n",
    "# Download\n",
    "files.download('tfidf_vectorizer.joblib')\n",
    "files.download('svm_classifier.joblib')\n",
    "files.download('label_encoder.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
